\documentclass[a4]{article}
\usepackage{graphicx}
\usepackage{color}
\usepackage[x11names,dvipsnames,table]{xcolor} %for use in color links

\usepackage{hyperref}
\newenvironment{review}%          environment name
{\textbf{Reviewer comment:}\begin{quote}}% begin code
{\end{quote}}%  
\newenvironment{answer}%          environment name
{\textbf{Answer:}\begin{small}\begin{quote}}% begin code
{\end{quote}\end{small}}%  

\newcommand{\todo}[1]{\color{red}#1\color{black}}

\newcommand{\revised}[1]{\color{blue} #1\color{black}}


\begin{document}

\section{Reviewer \#1}

\begin{review}
  The work presents a comprehensive overview on six of the most
  typical approaches that influenced the design of existing science
  gateways. The initial description of the individual architectures is
  indeed useful as background information leading to the realisation
  of a new gateway system. However, as the authors acknowledge, the
  different solutions may coexist in real implementations and the
  abstract perspectives provided are subject to a certain level of
  generality. In the opinion of the reviewer, the evaluation provided
  may result, in some cases, misleading for the the non-expert
  designer.
\end{review}

\begin{answer}

  We thank the reviewer for the thorough review and useful comments.

  The results and discussion reported in our paper are indeed aimed at
  an expert audience of science gateway and workflow engine
  developers. The evaluation metrics have been designed based on our
  long experience with both science gateways and workflow engines, and
  we completely acknowledge that non experts may misinterpret the
  results.

  The fact that the patterns may be mixed in real implementations is,
  in our opinion, another motivation for our abstraction effort as it
  helps ``untangling'' the various architectural concepts that may be
  present in a real system. This is particularly true for the abstract
  architectures (nested workflows and workflow conversion) that may be
  instantiated in various ways. Our abstractions highlight that other
  instantiation models may be worth designing, for instance nested
  workflows with task encapsulation, as mentioned in Section 5.2. in
  the Discussion.

  We added the following paragraph to the Introduction to reflect this discussion:

\revised{The analysis presented in the paper is intended for
experts in science gateway and workflow engine design. It is an
abstraction effort to identify and evaluate the fundamental
architectural patterns that are encountered while integrating workflow
engines and science gateways. In real systems, such patterns are often
intertwined due to considerations related to the historical and
technical context of software projects. The following work helps
identify the key patterns in systems and highlight potential
improvements.}

\end{answer}

\begin{review}
  Section 2: In the description of the Task Encapsulation pattern, it
  is not really clear what is intended as the interaction 'e' that
  envisage the submission of task to the science gateway. Are these
  tasks pure notifications about the progressing of the sub-task, or
  the actual workflow engine, deployed in the infrastructure, requires
  the gateway to directly control its own sub-tasks?
\end{review}

\begin{answer}
  The actual workflow engine deployed in the infrastructure
  requires the gateway to directly control its own sub-tasks. We
  revised the wording of Section 2.4 to clarify this point.
\end{answer}

\begin{review}
  Section 3: The evaluation is conducted by introducing a novel
  framework of metrics. It provides measurements based on the number
  of components of the gateway and the number of formal interactions
  required to satisfy four general characteristics: Robustness of
  Workflow execution, Extensibility, Scalability and Specific
  Features. Some aspects are a bit confusing. For instance, it is not
  immediately clear from the text, the difference between the
  Scalability metrics S1 and S2. Eventually they seem to differ,
  respectively, for the focus on dynamic elasticity of the former,
  versus the pre-hoc allocation of multiple workflow instances of the
  latter.
\end{review}

\begin{answer}
  S1 denotes the ability of the system to start multiple engine
  instances to execute different workflows, while S2 denotes the
  possibility to execute a single workflow with multiple engines. We
  revised the wording in the description of S1 (page 11) to clarify
  this point. We find the current description of S2 clear enough on
  this point: ``[S2] measures the ability to distribute the execution of
  \emph{a single workflow} among different engine instances.''
\end{answer}

\begin{review}
The metric E3, measuring the complexity of integrating more workflows applications, seems to be more functional rather than architectural. The implications and impacts on the formal interactions required to support this feature across patterns are very similar.
 On the other hand, is opinion of the reviewer that integrating new workflow applications would rather affect more the Robustness of the system, especially in the case of the Workflow Conversion pattern. In this context, a formal conversion interaction (g), if not validated, could lead to logical or system errors.
\end{review}

\begin{answer}
  We introduced E3 precisely to account for the additional
  complexity introduced by interaction (g) when adding new
  workflows. 
  
  It should be understood here that workflow conversion is
  presented as a pattern to integrate workflow \emph{engines} in a
  science gateway, through workflow translation to a language already
  supported by the gateway. We believe that integrating new workflow
  engines (therefore new workflows) through workflow conversion is a
  strong architectural design choice that may, depending on the
  availability of a reliable interaction (g), be avoided by
  integrating workflow engines using a different pattern. We added the following sentence at the beginning of Section 2.7 to explain that:
  
  \revised{Workflow conversion is presented here as a pattern to
integrate workflow \emph{engines} in a science gateway, through
workflow format conversion from a native format to the science gateway
format.}


We definitely agree with the reviewer that interaction (g) is
  a tricky one that may lead to critical errors. We added the
  following sentence to the paragraph on interaction (g) in Section
  5.2. of the discussion:

\revised{If not properly validated, workflow language conversion
  could introduce critical errors impacting the robustness and
  correctness of the execution. }

This sentence complements the note already made in section 3.7:
``Note, however,
that implementing interaction g can require sub-
stantial work depending on the complexity of the
workflow language used by the new engine.``
\end{answer}

\begin{review}
 Given these observations and the other measurements provided by the paper, the overall performance presented in Table 3, where the Workflow Conversion outperforms the Service Invocation pattern, is considered counter intuitive. The result is justified though by the evaluation of the Service-invocation pattern.
\end{review}

\begin{answer}

  As discussed previously, we share the reviewer's concerns
  about the feasibility of workflow conversion (see comments on
  interaction (g) in Section 5.2). The performance of workflow
  conversion is strongly related to the availability of a validated
  interaction (g), which is very dependent on the particular workflow
  engines under consideration. We believe that the issue raised by the
  reviewer basically comes down to the weight given to software
  components and interactions in the analysis. However, using
  different weights for different components or interactions would
  open an infinite number of configurations for discussion. For
  instance, in some cases, the workflow engine already comes with a
  workflow service which therefore doesn't have to be included. To
  address this issue, we shared a spreadsheet at
  \url{https://frama.link/LrPRsCQp} that allows editing the following
  weights:
  \begin{itemize}
  \item Weight given to software components and interactions in
  ``integration'', ``extensibility'' and ``robustness'' criteria.
  \item Weight given to metrics in a criterion (e.g., E$_1$ vs. E$_2$ in
  ``extensibility'').
  \item Weight given to criteria in overall evaluation of Table 3.
\end{itemize}
The paper references this spreadsheet wherever equal weights are
assumed. The readers and reviewers are invited to rely on this
spreadsheet for any issue related to the importance given to a
specific component, interaction, metric or criterion in the
analysis. In the paper, we assumed that all weights equal 1, which we
believe is a reasonable choice although inherently limited.

\end{answer}

\begin{review}
  Here, a high value (low score) is assigned to the Extensibility
  metrics E1, concerning the inclusion of new workflow engines. This
  seems to be too negative, since a service invocation approach can
  rely on generic interfaces, re-purposing via a common service layer
  supporting multiple technologies.
\end{review}

\begin{answer}

The reviewer is right that re-purposing software components
  and interactions can lead to revisit the evaluation. We updated the following sentence in section 5.2. to better acknowledge this point:

The particular technical or historical context of a science gateway
project may also influence the choice of an architecture to
integrate workflow engines. For instance, many workflow engines are
already available as web services, which tends to favor service
invocation (\revised{in particular when interactions can be re-purposed}), and other science gateways may have strongly tested task
and data control features (interactions \texttt{b} and \texttt{c}),
which would favor task encapsulation. Similarly, adding a new type of engine
may facilitate integration of a new infrastructure when interactions
\texttt{b} and \texttt{c$_2$} are already available. The migration
cost between architectures has been ignored as well.

In the specific case of service invocation, we agree with the reviewer
that, when adding a new workflow engine, the workflow service or interaction \texttt{d} can always be repurposed depending on the context. We decremented E1 and added the following explanation in section 3.3.:

\revised{However, when a new type of workflow engine is added, it is always possible to reuse either an existing available workflow service or interaction \texttt{d} (when a new workflow service has to be developed). Therefore, \texttt{E$_1$}=3}.
\end{answer}

\begin{review}
 This is reflected also in the negative evaluation of the S1,S2 metrics for the same pattern. On the contrary, the choice of this architecture would intuitively suggest re-usable, scalable and decoupled solutions.
\end{review}

\begin{answer}
  While we agree with the reviewer that re-usability should
  definitely be taken into account in E$_1$, we do not see how it
  could help S$_1$ and S$_2$.

  For S$_1$, starting new instances of the workflow service still
  requires that a specific mechanism is implemented in the science
  gateway to distribute the load among these services, even if all
  these services share the same interface. Again, the difficulty to
  implement such mechanisms could be extensively discussed: some
  systems may rely on a simple DNS-level load balancing, but this
  would not cope with the fact that workflow executions have different
  resource requirements and that workflow services may have different
  availabilities (for instance, a workflow service may crash in case a
  workflow execution filled up its disk); other systems may balance
  the load based on resource usage (e.g. RAM, CPU and disk) in the
  workflow services, but this requires an elaborated monitoring
  infrastructure. On the contrary, adding new engine instances is
  naturally supported by task encapsulation and pool, which justifies
  that S$_1=0$ for these models while S$_1$=1 for service invocation.

  For S$_2$, we cannot think of a practical reason why the service
  invocation model would help distribute a single workflow execution
  in multiple engine instances. In our opinion, this feature is
  exclusive to the nested workflow model.
\end{answer}

\begin{review}
 Eventually, the arguments presented were not considered convincing.
\end{review}

\begin{answer}
We hope that the update made in the extensibility scores and the
explanations given for scalability convinced the reviewer.
\end{answer}

\begin{review}
  Again on the definition of Robustness. It introduces two metrics:
  Specific Components and Interactions. These are explained in terms
  of what they not include but no evidence or examples are provided to
  better understand what is intended specific. 
\end{review}

\begin{answer}
We updated the definition of R$_1$ and R$_2$ to include
  positive examples as well as negative ones. In addition, the
  specific components included in these metrics are represented in red
  on Figure 2.
\end{answer}

\begin{review}
Moreover, some
  interactions which are mentioned as being not specific, become
  specific during the evaluation. For instance, interaction 'c', which
  is defined as the upload of input data and results (c1) and to
  transfer processed data across infrastructures (c2), are initially
  considered as an example of not specific interaction. Though, when
  measuring the Robustness of the Pool Model, c2 suddenly appears in
  the count, 
\end{review}

\begin{answer}
\texttt{c}, \texttt{c$_1$} and \texttt{c$_2$} are three different
interactions, which explains the confusion. \texttt{c$_2$} is always
specific to workflow execution because it is defined as the
interaction between the workflow engine and the infrastructure, used
to transfer workflow data (inputs, outputs or intermediate
results). \texttt{c$_1$} is not specific to workflow execution because
it is used by the science gateway to transfer any data regardless of
workflow executions. Letter \texttt{c} was used on the task
encapsulation diagram in Figure 2 to represent that the science gateway transfers
both user and workflow data. We now use \texttt{c$_1$} for all the
data transfers initiated by the science gateway, and \texttt{c$_2$}
for all the data transfers initiated by the workflow engine,
regardless of the type of data transferred. Consequently, we now use
\texttt{c$_1$} instead of \texttt{c} on the task encapsulation diagram in Figure 2.

The ambiguity is now removed and the new definition of R$_1$ and R$_2$ reads as follows:

\begin{itemize}
\item Specific components (\texttt{R$_1$}): number of specific components involved in workflow executions. \revised{For instance, the
  workflow service is a specific component because its main objective
  is to execute workflows. On the contrary, the science gateway is
  \emph{not} specific to workflow execution since it is used for a
  variety of other functions such as user authentication, data
  transfer, etc.}
\item Specific interactions (\texttt{R$_2$}): number of specific
  interactions involved in workflow executions. \revised{For
    instance, the pool-agent interaction (interaction \texttt{f}) is
    specific to workflow execution while data control (interaction
    \texttt{c$_1$}) is \emph{not} because it is used by the science gateway to transfer
    user data regardless of a particular workflow execution}.
\end{itemize}
\end{answer}

\begin{review}
Does the definition of R2 means that non-specific
  interactions should be counted?
\end{review}

\begin{answer}
No it doesn't. We hope that the new examples in the definition of
R$_1$ and R$_2$ and the removal of any reference to interaction
\texttt{c} clarifies this point.
\end{answer}

\begin{review}
The higher number the worse
  robustness?
\end{review}

\begin{answer}
Yes, as for any other metric, high values indicate worse
performance. This is explained at the beginning of section 3:
``Criteria break down into specific metrics where lower value
indicates better performance.''
\end{answer}

\begin{review}
It is not always obvious that Scalability could be badly affected in a Tight Integration. The example of a liferay portlet is actually potentially demonstrating the opposite. Portlets can be independent web application that can be even remotely deployed (via WSRP). The integration of a workflow engine via ad-hoc portlet applications could instead guarantee quite of a level of decoupling from the rest of the gateway. Here, the limitations highlighted seems to be a bit superficial.
\end{review}

\todo{think of this}

\begin{review}
The "Specific Feature", referring to the Fine-grained debugging does not specify what is relevant to be monitored and who is going to gain benefit from the monitoring. The scientists, the gateway developers or the administrators? What is a white box, the workflow engine or the workflow task? Why multiple software layers affect the accuracy of the tasks' internals?
\end{review}

\begin{answer}
  We call ``white-box'' workflow a workflow
  where detailed information about individual tasks is available,
  for instance task statuses, execution logs and resource
  consumption. On the contrary, a ``black-box'' workflow only
  displays global information such as the workflow status and the
  logs of the workflow engine. Fine-grained information about
  workflow tasks helps workflow users (scientists)
  identify errors, it allows gateway administrators to troubleshoot
  executions, and it facilitates debugging by workflow
  developers.
  
  Multiple software layers affect the accuracy of the information
  reported about the tasks' internals because each layer potentially
  shields or modifies information. Task ids, for instance, may not
  be properly linked to workflow activities if delivered through a
  software layer that hasn't been instrumented for that. \todo{refer
    to olabarriaga2014}.
  
  The paragraph on fine-grained debugging has been edited accordingly, at the end of Section 3.
\end{answer}

\begin{review}
To conclude, the general feeling is that the definition of the metrics and the interaction patterns are not rigorous enough to provide an interpretation of the final evaluation which is entirely useful for the reader.
\end{review}

\begin{answer}
We hope that the explanations provided in this letter and the updated text in the paper demonstrate that the metrics and definitions are rigorous.
\end{answer}

\begin{review}
 Probably, some more information on how "Future Generation" systems should be designed, starting from the general overview provided, with more technical understanding of the system complexities, could represent a more valuable contribution to the main aim of this particular publication.
\end{review}

\begin{answer}
  We added a section in the paper (current section 6) to show
  how our evaluation framework could be used in the design of new
  systems.
\end{answer}

\begin{review}
To improve the reading, Dedicating separate sections to the introduction of the metrics and the evaluation of each pattern would be beneficial. For instance Section 3 Metrics, Section 4 Evaluation.
\end{review}

\begin{answer}
This was done as suggested.
\end{answer}

\section{Reviewer \#2}

\begin{review}
  This work describes and evaluates six software architectures
  commonly used to integrate workflow engines into science gateways
  with metrics for assessment of integration complexity, robustness,
  extensibility, scalability and support for meta-workflows and
  fine-grained debugging. The paper is well written in general, and
  the motivation is very nice, clearly comparing the different types
  of architectures that can be found at the back end of a science
  gateway. However, the work has a list of issues that could be
  addressed by the authors:

1. Apache Araviate and WS-PGRADE/gUSE are not science gateways. They
are open source science gateway framework that enables users to access
computing infrastructures frameworks. Therefore, in the Table 1, where
the classifications of science gateways are displayed, it should list
the science gateways that have used those architectures instead of the
frameworks. Otherwise, is quite confusing.
\end{review}

\todo{just remove CSGF, Airvata and pgrade from the table, keeping the
  exponents. But we don't have examples of gateways using Airvata,
  still need to be found.}

\begin{review}
  2. Tight Architecture -→ What happens if the workflow engine is
  built into the Science Gateway, but still need a submission
  framework (e.g. WS-PGRADE/gUSE or SAGA) for submitting the enteir
  workflow to the infrastructure.  It would be a tight architecture or
  a task architecture ? I can not see this type of architecture
  reflected in the paper.
\end{review}

\begin{answer}
  The task submission framework is part of the infrastructure
  and its nature doesn't influence the architecture used to integrate
  workflow engines in the science gateway. Formally, it is part of
  interaction \texttt{b} (see definition in 2.1).

  If the ``entire workflow'', i.e. the complete control and data flow,
  is submitted to the infrastructure through interaction \texttt{b} as
  a single task that will manage the execution and potentially submit
  sub-tasks, then this is a task encapsulation architecture. In
  particular, a dedicated engine instance can easily be started for
  every workflow execution.

  If only the workflow tasks are submitted to the infrastructure
  through interaction \texttt{b}, and the main workflow logic is
  controlled from within the science gateway, then this is clearly a
  tight integration.

  The reviewer's use case, however, suggest that the workflow engine
  is split in two parts: (1) a high-level part integrated in the
  science gateway, which submits ``entire workflows'' to the
  infrastructure; (2) a lower-level part, executed on the
  infrastructure, which executes such ``entire workflows''. It seems
  to us that this matches the definition of a nested workflow model,
  where the parent engine is integrated in the science gateway, and
  the child engine runs on the infrastructure. This is another
  instantiation of the nested workflow abstract model, which is not
  evaluated in the paper as mentioned in section 5.2: "\emph{The
    abstract nested workflows and workflow conversion patterns were
    instantiated with service invocation so that they can be analyzed
    in the same framework as the other architectures. Other types of
    instantiation, for instance nested workflows with task
    encapsulation, could also be envisaged.}". It sounds like the
  architecture described by the reviewer is a nested workflow pattern
  instantiated with a tight integration (parent engine) and task
  encapsulation (child engine).

  We found this scenario interesting and we evaluated it in our
  framework. The analysis is reported in the new section 6 of the
  paper.
\end{answer}

\begin{review}
  3. Why there isn't a figure to illustrate an example of a real
  science gateway that corresponds to this architecture, like the
  other types of archtictures (e.g. Figure 3, Figure 4)
\end{review}

\begin{review}
  4. In my opininion Figures 3, 4, 5, 6, should use the iteractions
  explained in the subction 2.1, instead the steps/calls, to be more
  clear. Right now, they are difficult to read.
\end{review}

\begin{review}
  5.  Subsection 2.4, "see for instrance attribute depend of option -W
  torque"-→ repharase, and explain it better.
\end{review}

\begin{review}
6.  Figures 4, 5, 6 are very difficult to understand. Maybe will be a
good idea to create a subsection for explaining them better and reduce
the captions of those figures.
\end{review}

\begin{answer}
As suggested by the reviewer, the presentation of these Figures has been gathered in a separate section, the new section 3.
\end{answer}

\begin{review}
  7.  Metrics: The metrics that have been used in this paper favour
  the simplest architectures. For example, looking at the Robustnes
  metrics, R1 and R2, they measure the probability that a workflow
  fails due to the errors of components and interactions that contains
  each architecture. But those metrics do not measure how "grave" are
  those failures and how "difficult" are those failures to repair. The
  same thing for the "Extensibility" metrics, some components can be
  much more difficult to "replace" than others.  Maybe would be a good
  idea to ponderete those metrics based on the number of components or
  interactions (e.g E.1---> dividing the number of interactions or
  components that need to be modified to integrate a new workflow
  between the total number of interactions or components. )
\end{review}

\begin{answer}
  We agree with the reviewer that different weights could be given to
  interactions and software components depending on the ``gravity'' of
  the associated failures, the ``difficulty'' to repair them and even
  the difficulty to implement such interactions and components in
  practice (for instance, interaction \texttt{g} is clearly very
  difficult to implement).

  However, we believe that assigning such weights is specific to the
  case of a particular system, i.e. a science gateway and workflow
  engine(s). Very different ponderations could be obtained depending
  on the technical context, for instance implementation language, type
  of targeted infrastructure (dedicated cluster vs. grid vs. cloud)
  and characteristics of the workflow engine(s) and language(s).

  Instead, we shared a spreadsheet at
  \url{https://frama.link/LrPRsCQp} that allows editing the following
  weights (see also answer to Reviewer \#1):
  \begin{itemize}
  \item Weight given to software components and interactions in
  ``integration'', ``extensibility'' and ``robustness'' criteria.
  \item Weight given to metrics in a criterion (e.g., E$_1$ vs. E$_2$ in
  ``extensibility'').
  \item Weight given to criteria in overall evaluation of Table 3.
\end{itemize}
The paper references this spreadsheet wherever equal weights are
assumed. The readers and reviewers are invited to rely on this
spreadsheet for any issue related to the importance given to a
specific component, interaction, metric or criterion in the
analysis. In the paper, we assumed that all weights equal 1, which we
believe is a reasonable choice although inherently limited.

We find the particular ponderation method suggested by the reviewer
(normalization by the number of components or interaction in the
architecture) a bit unfortunate because it would give different
ponderations to the same component or interaction depending on the
architecture in which it is used, which we think would obfuscate the
comparison.
\end{answer}

\begin{review}
8. In my opinion, if the workflow it is integrated in the science
gateway (tight architecture), then the science gateway should be
evaluated as a component, since it also could fail. The same thing for
the task encapsulation and the infrastructure, in that case, the
Infraestructure should be also considered as component. So, I would
modifiy the table 2, according to that ( e.g., R1=1 for Tight and Task
architecture).
\end{review}

\begin{answer}
  We believe that the robustness of a system for a particular function
  should be evaluated w.r.t the set of components and interactions
  that are \emph{specific} to this function. Indeed, the failure of
  non-specific components is likely to impact many other functions
  that are required by the particular function of interest. To take an
  extreme example, a failure of the internet connection of the gateway
  user would certainly prevent him/her to launch a new workflow, but
  it does not really make sense to integrate the internet connection
  in the robustness analysis because it is not specific to workflow
  execution. Similarly, the failure of the science gateway would
  prevent users to login in the first place, which would prevent
  workflow execution in any case. Likewise, the failure of the
  infrastructure would make any workflow execution worthless in any
  architecture because infrastructure is used for other critical
  functions such as task execution and data storage.

  For these reasons, we disagree with the suggestion made by the
  reviewer. In any case, science gateway and infrastructure are
  present in all the architectures, which would only add 2 to all the
  values of R1.
\end{answer}

\begin{review}
9. According with Figure 2.I, this workflow architecture has 3
interactions b, c1 and c2, since data movement can be done by the
users and workflows in this type of architecture (those interactions
are not exlcuyents) so R2=3
\end{review}

\begin{answer}
  In Figure 2.I, interaction \texttt{c$_1$} is not specific to
  workflow execution since it is used to transfer any data to the
  infrastructure. This is why we don't count it in R2, for the same
  reason as explained previously.
\end{answer}

\begin{review}
  10. The text of Sections 3.2, 3.3, 3.4, 3.5, 3.6 and 3.7 could be
  reduced, since most of the values are obvious from Figures 2.II and
  Table 2.
\end{review}

\begin{answer}
  While we agree that the Sub-Sections in Section 3 look a bit
  repetitive, we believe that the metrics evaluation reported in Table 2 needs to be
  properly justified. 
\end{answer}

\begin{review}
  11. Service invocation and Task encapsulation can also be the same
  thing
\end{review}

\begin{answer}
  Service invocation and Task encapsulation are fundamentally different for the following reasons:
  \begin{itemize}
  \item In task encapsulation, the workflow engine runs on the infrastructure while in service invocation, it is hosted by a dedicated service.
  \item In task encapsulation, the workflow engine is executed as a task of the infrastructure while in service invocation, it is invoked through a service call. That means, in particular, that in task encapsulation different workflow engines are executed through the same protocol (a task), while different types of service calls might be involved in service invocation (e.g. REST or SOAP Web-Service).
  \item In task encapsulation, the workflow engine does not transfer data to the infrastructure while in service invocation it does (interactioni \texttt{c$_2$}.
  \item In task encapuslation, the science gateway submits tasks to the infrastructure, while in service invocation the workflow engine does.
  \end{itemize}
  Therefore, while we acknowledge that task encapsulation and service
  invocation share characteristics, we believe that their differences
  can be clearly inferred from section 2 of the paper. 
\end{answer}

\begin{review}
12. No so sure that tight integration are the most easy for
integrating a workflow engine, since in case a the workflow engine
needs to be modified, the whole science gateway has to be change
(e.g. re-programe a portlet in Liferay). On the other hand, with
Service invocation, the replacement could be very much more easier.
\end{review}

\todo{ponderation again}

\section{Reviewer \#3}

\begin{review}
This paper provides a comparison of 6 different software architectures commonly used to integrate workflow engines into science gateways. For this comparison, the authors define seven different types of interactions to outline the differences in approach. The combination of components and interactions for the 6 overview models in Figure 2 that form the underlying basis for the comparisons.

The authors then provide a set of Metrics: complexity, robustness, extensibility, scalability \& specific features to compare and contrast the different software architectures.

Complexity, for example, is measured by counting the number of components and interactions that were defined and annotated in Figure 2.  Extensibility measures the difficulty to replace or add elements to the architecture e.g. a workflow engine, an engine version update, add a new workflow or add new infrastructure. 

I liked this paper but I also struggled a lot with it too because ultimately, whilst this metric structure provides some rudimentary structure to quantify the various software architectures, a many software architecture perspectives are missing, and the results are not normalized against the functionality a system provides.

For example, when you normally think of software architectures, a good design is often one that is layered and componentized. There are multiple benefits of such an an approach over monolithic systems but using the metrics defined in this paper, good software practices to expose functionality is not really factored in. So, the result is that the more layers to components in the system, the worse it scores. So, these aspects of good software engineering or providing enhanced functionality seem penalized in this paper, as it stands. 
\end{review}

\begin{review}
An example where the logic fails I believe is in the Extensibility. Let's take two cases initially:
-	Pool architecture
-	Nested.
For pool this score is 9 but for nested the score is 12.5. So, using these metrics the pool architecture is easier to extend.  But, if this is based on the same interface (e.g. SHIWA), then the difficulty to extend each system seems quite similar, because basically a workflow engine needs to implement the SHIWA course grained ORE interface to allow it to work. If it implements this interface as a client and a server then it can both consume other workflows and be embedded by them. For the pool architecture in the most generic sense, a workflow would need to do both (to digest ORE bundles and to pass them on), and for a nested architecture, a workflow engine would need to do one of the other depending on whether it was calling another workflow or being invoked by one.  However, as I understand, the case here assumes there is no connectivity (passing of bundles) in the pool case, so it seems less complex to extend. So, in the end, it is not about how complex the extension is
it is about what you want to do.  For example, for the nested case, you could always assume one master workflow, then it would be simpler (but less flexible). So in the end this table could look very different depending on how the different systems are defined in scope, and you could potentially have various versions of either of these cases. 
\end{review}

\todo{answer metric by metric, focusing on E1 and E4. For E4 it's clear. For E1, }

\begin{review}
Adding functionality e.g. in this case to add the ability to pass bundles to one system and another is a positive (and complex) thing to accomplish. There should be a way of counting this and having this score higher than Tight, Service or task, or normalizing this functionality into the metrics to provide a more balanced view. The "Scalability" touches on this but "Functionality" could be another aspect here to consider.
\end{review}

\begin{review}
Another example is for complexity.  Again, the less components and interactions the system has the less complex it is. But in practice, it is not so simple. For example, the tight integration is where the workflow system is integration directly into the science gateway and into the infrastructure. Whereas this architecture has the least number of components and interactions of them all it does not mean it is less complex in practice. Integrating a workflow into science gateway and directly on top of the infrastructure depends very much on the tools they sit upon and the complexity of that integration. Such tight integrated systems therefore could be massive monolithic pieces of code that are extremely difficult and complex to develop and maintain.  Whereas a task-based approach or service based approach through breaking code out to different services and tasks may be far simpler to develop and maintain. 
\end{review}

\todo{complexity is used as a measure of reliability}

\begin{review}
I therefore think that the metrics defined only provide a very high level structure for an evaluation of the different software architectures and they should discuss other factors and scenarios that can affect development. Some of these limitations are discussed in section 4.2 but I think an extra section discuss these before the definition of the metrics would be very helpful to set the scene correctly for the evaluation. Otherwise, the reader is left asking a number of questions during the evaluation section. I also think that a few more metrics e.g. "functionality" could be added to provide a more balanced view of the different approaches. Another useful extension of the paper could be to normalize the results against the functionality each approach provides (perhaps in a separate table) - this may help to provide a more balanced table that factors in functionality.
\end{review}

\end{document}