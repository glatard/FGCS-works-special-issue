\documentclass[a4]{article}
\usepackage{graphicx}
\usepackage{color}
\usepackage[x11names,dvipsnames,table]{xcolor} %for use in color links

\usepackage{hyperref}
\usepackage{xspace}
\newenvironment{review}%          environment name
{\textbf{Reviewer comment:}\begin{quote}}% begin code
{\end{quote}}%  
\newenvironment{answer}%          environment name
{\textbf{Answer:}\begin{small}\begin{quote}}% begin code
{\end{quote}\end{small}}%  

\newcommand{\todo}[1]{\color{red}#1\color{black}}

\newcommand{\revised}[1]{\color{blue} #1\color{black}\xspace}


\begin{document}

\section{Reviewer \#1}

\begin{review}
  The work presents a comprehensive overview on six of the most
  typical approaches that influenced the design of existing science
  gateways. The initial description of the individual architectures is
  indeed useful as background information leading to the realisation
  of a new gateway system. However, as the authors acknowledge, the
  different solutions may coexist in real implementations and the
  abstract perspectives provided are subject to a certain level of
  generality. In the opinion of the reviewer, the evaluation provided
  may result, in some cases, misleading for the the non-expert
  designer.
\end{review}

\begin{answer}

  We thank the reviewer for the thorough review and useful comments.

  The results and discussion reported in our paper are indeed aimed at
  an expert audience of science gateway and workflow engine
  developers. The evaluation metrics have been designed based on our
  long experience with both science gateways and workflow engines, and
  we completely acknowledge that non experts may misinterpret the
  results.

  The fact that the patterns may be mixed in real implementations is,
  in our opinion, another motivation for our abstraction effort as it
  helps ``untangling'' the various architectural concepts that may be
  present in a real system. This is particularly true for the abstract
  architectures (nested workflows and workflow conversion) that may be
  instantiated in various ways. Our abstractions highlight that other
  instantiation models may be worth designing, for instance nested
  workflows with task encapsulation, as mentioned in Section 5.2. in
  the Discussion.

  We added the following paragraph to the Introduction to reflect this discussion:

\revised{The analysis presented in the paper is intended for
experts in science gateway and workflow engine design. It is an
abstraction effort to identify and evaluate the fundamental
architectural patterns that are encountered while integrating workflow
engines and science gateways. In real systems, such patterns are often
intertwined due to considerations related to the historical and
technical context of software projects. The following work helps
identify the key patterns in systems and highlight potential
improvements.}

\end{answer}

\begin{review}
  Section 2: In the description of the Task Encapsulation pattern, it
  is not really clear what is intended as the interaction 'e' that
  envisage the submission of task to the science gateway. Are these
  tasks pure notifications about the progressing of the sub-task, or
  the actual workflow engine, deployed in the infrastructure, requires
  the gateway to directly control its own sub-tasks?
\end{review}

\begin{answer}
  The actual workflow engine deployed in the infrastructure
  requires the gateway to directly control its own sub-tasks. We
  revised the wording of Section 2.4 to clarify this point.
\end{answer}

\begin{review}
  Section 3: The evaluation is conducted by introducing a novel
  framework of metrics. It provides measurements based on the number
  of components of the gateway and the number of formal interactions
  required to satisfy four general characteristics: Robustness of
  Workflow execution, Extensibility, Scalability and Specific
  Features. Some aspects are a bit confusing. For instance, it is not
  immediately clear from the text, the difference between the
  Scalability metrics S1 and S2. Eventually they seem to differ,
  respectively, for the focus on dynamic elasticity of the former,
  versus the pre-hoc allocation of multiple workflow instances of the
  latter.
\end{review}

\begin{answer}
  S1 denotes the ability of the system to start multiple engine
  instances to execute different workflows, while S2 denotes the
  possibility to execute a single workflow with multiple engines. We
  revised the wording in the description of S1 (page 11) to clarify
  this point. We find the current description of S2 clear enough on
  this point: ``[S2] measures the ability to distribute the execution of
  \emph{a single workflow} among different engine instances.''
\end{answer}

\begin{review}
The metric E3, measuring the complexity of integrating more workflows applications, seems to be more functional rather than architectural. The implications and impacts on the formal interactions required to support this feature across patterns are very similar.
 On the other hand, is opinion of the reviewer that integrating new workflow applications would rather affect more the Robustness of the system, especially in the case of the Workflow Conversion pattern. In this context, a formal conversion interaction (g), if not validated, could lead to logical or system errors.
\end{review}

\begin{answer}
  We introduced E3 precisely to account for the additional
  complexity introduced by interaction (g) when adding new
  workflows. 
  
  It should be understood here that workflow conversion is
  presented as a pattern to integrate workflow \emph{engines} in a
  science gateway, through workflow translation to a language already
  supported by the gateway. We believe that integrating new workflow
  engines (therefore new workflows) through workflow conversion is a
  strong architectural design choice that may, depending on the
  availability of a reliable interaction (g), be avoided by
  integrating workflow engines using a different pattern. We added the following sentence at the beginning of Section 2.7 to explain that:
  
  \revised{Workflow conversion is presented here as a pattern to
integrate workflow \emph{engines} in a science gateway, through
workflow format conversion from a native format to the science gateway
format.}


We definitely agree with the reviewer that interaction (g) is
  a tricky one that may lead to critical errors. We added the
  following sentence to the paragraph on interaction (g) in Section
  5.2. of the discussion:

\revised{If not properly validated, workflow language conversion
  could introduce critical errors impacting the robustness and
  correctness of the execution. }

This sentence complements the note already made in section 3.7:
``Note, however,
that implementing interaction g can require sub-
stantial work depending on the complexity of the
workflow language used by the new engine.``
\end{answer}

\begin{review}
 Given these observations and the other measurements provided by the paper, the overall performance presented in Table 3, where the Workflow Conversion outperforms the Service Invocation pattern, is considered counter intuitive. The result is justified though by the evaluation of the Service-invocation pattern.
\end{review}

\begin{answer}

  As discussed previously, we share the reviewer's concerns
  about the feasibility of workflow conversion (see comments on
  interaction (g) in Section 5.2). The performance of workflow
  conversion is strongly related to the availability of a validated
  interaction (g), which is very dependent on the particular workflow
  engines under consideration. We believe that the issue raised by the
  reviewer basically comes down to the weight given to software
  components and interactions in the analysis. However, using
  different weights for different components or interactions would
  open an infinite number of configurations for discussion. For
  instance, in some cases, the workflow engine already comes with a
  workflow service which therefore doesn't have to be included. To
  address this issue, we shared a spreadsheet at
  \url{https://frama.link/LrPRsCQp} that allows editing the following
  weights:
  \begin{itemize}
  \item Weight given to software components and interactions in
  ``integration'', ``extensibility'' and ``robustness'' criteria.
  \item Weight given to metrics in a criterion (e.g., E$_1$ vs. E$_2$ in
  ``extensibility'').
  \item Weight given to criteria in overall evaluation of Table 3.
\end{itemize}
The paper references this spreadsheet wherever equal weights are
assumed. The readers and reviewers are invited to rely on this
spreadsheet for any issue related to the importance given to a
specific component, interaction, metric or criterion in the
analysis. In the paper, we assumed that all weights equal 1, which we
believe is a reasonable choice although inherently limited.

\end{answer}

\begin{review}
  Here, a high value (low score) is assigned to the Extensibility
  metrics E1, concerning the inclusion of new workflow engines. This
  seems to be too negative, since a service invocation approach can
  rely on generic interfaces, re-purposing via a common service layer
  supporting multiple technologies.
\end{review}

\begin{answer}

The reviewer is right that re-purposing software components
  and interactions can lead to revisit the evaluation. We updated the following sentence in section 5.2. to better acknowledge this point:

The particular technical or historical context of a science gateway
project may also influence the choice of an architecture to
integrate workflow engines. For instance, many workflow engines are
already available as web services, which tends to favor service
invocation (\revised{in particular when interactions can be re-purposed}), and other science gateways may have strongly tested task
and data control features (interactions \texttt{b} and \texttt{c}),
which would favor task encapsulation. Similarly, adding a new type of engine
may facilitate integration of a new infrastructure when interactions
\texttt{b} and \texttt{c$_2$} are already available. The migration
cost between architectures has been ignored as well.

In the specific case of service invocation, we agree with the reviewer
that, when adding a new workflow engine, the workflow service or interaction \texttt{d} can always be repurposed depending on the context. We decremented E1 and added the following explanation in section 3.3.:

\revised{However, when a new type of workflow engine is added, it is always possible to reuse either an existing available workflow service or interaction \texttt{d} (when a new workflow service has to be developed). Therefore, \texttt{E$_1$}=3}.
\end{answer}

\begin{review}
 This is reflected also in the negative evaluation of the S1,S2 metrics for the same pattern. On the contrary, the choice of this architecture would intuitively suggest re-usable, scalable and decoupled solutions.
\end{review}

\begin{answer}
  While we agree with the reviewer that re-usability should
  definitely be taken into account in E$_1$, we do not see how it
  could help S$_1$ and S$_2$.

  For S$_1$, starting new instances of the workflow service still
  requires that a specific mechanism is implemented in the science
  gateway to distribute the load among these services, even if all
  these services share the same interface. Again, the difficulty to
  implement such mechanisms could be extensively discussed: some
  systems may rely on a simple DNS-level load balancing, but this
  would not cope with the fact that workflow executions have different
  resource requirements and that workflow services may have different
  availabilities (for instance, a workflow service may crash in case a
  workflow execution filled up its disk); other systems may balance
  the load based on resource usage (e.g. RAM, CPU and disk) in the
  workflow services, but this requires an elaborated monitoring
  infrastructure. On the contrary, adding new engine instances is
  naturally supported by task encapsulation and pool, which justifies
  that S$_1=0$ for these models while S$_1$=1 for service invocation.

  For S$_2$, we cannot think of a practical reason why the service
  invocation model would help distribute a single workflow execution
  in multiple engine instances. In our opinion, this feature is
  exclusive to the nested workflow model.
\end{answer}

\begin{review}
 Eventually, the arguments presented were not considered convincing.
\end{review}

\begin{answer}
We hope that the update made in the extensibility scores and the
explanations given for scalability convinced the reviewer.
\end{answer}

\begin{review}
  Again on the definition of Robustness. It introduces two metrics:
  Specific Components and Interactions. These are explained in terms
  of what they not include but no evidence or examples are provided to
  better understand what is intended specific. 
\end{review}

\begin{answer}
We updated the definition of R$_1$ and R$_2$ to include
  positive examples as well as negative ones. In addition, the
  specific components included in these metrics are represented in red
  on Figure 2.
\end{answer}

\begin{review}
Moreover, some
  interactions which are mentioned as being not specific, become
  specific during the evaluation. For instance, interaction 'c', which
  is defined as the upload of input data and results (c1) and to
  transfer processed data across infrastructures (c2), are initially
  considered as an example of not specific interaction. Though, when
  measuring the Robustness of the Pool Model, c2 suddenly appears in
  the count, 
\end{review}

\begin{answer}
\texttt{c}, \texttt{c$_1$} and \texttt{c$_2$} are three different
interactions, which explains the confusion. \texttt{c$_2$} is always
specific to workflow execution because it is defined as the
interaction between the workflow engine and the infrastructure, used
to transfer workflow data (inputs, outputs or intermediate
results). \texttt{c$_1$} is not specific to workflow execution because
it is used by the science gateway to transfer any data regardless of
workflow executions. Letter \texttt{c} was used on the task
encapsulation diagram in Figure 2 to represent that the science gateway transfers
both user and workflow data. We now use \texttt{c$_1$} for all the
data transfers initiated by the science gateway, and \texttt{c$_2$}
for all the data transfers initiated by the workflow engine,
regardless of the type of data transferred. Consequently, we now use
\texttt{c$_1$} instead of \texttt{c} on the task encapsulation diagram in Figure 2.

The ambiguity is now removed and the new definition of R$_1$ and R$_2$ reads as follows:

\begin{itemize}
\item Specific components (\texttt{R$_1$}): number of specific components involved in workflow executions. \revised{For instance, the
  workflow service is a specific component because its main objective
  is to execute workflows. On the contrary, the science gateway is
  \emph{not} specific to workflow execution since it is used for a
  variety of other functions such as user authentication, data
  transfer, etc.}
\item Specific interactions (\texttt{R$_2$}): number of specific
  interactions involved in workflow executions. \revised{For
    instance, the pool-agent interaction (interaction \texttt{f}) is
    specific to workflow execution while data control (interaction
    \texttt{c$_1$}) is \emph{not} because it is used by the science gateway to transfer
    user data regardless of a particular workflow execution}.
\end{itemize}
\end{answer}

\begin{review}
Does the definition of R2 means that non-specific
  interactions should be counted?
\end{review}

\begin{answer}
No it doesn't. We hope that the new examples in the definition of
R$_1$ and R$_2$ and the removal of any reference to interaction
\texttt{c} clarifies this point.
\end{answer}

\begin{review}
The higher number the worse
  robustness?
\end{review}

\begin{answer}
Yes, as for any other metric, high values indicate worse
performance. This is explained at the beginning of section 3:
``Criteria break down into specific metrics where lower value
indicates better performance.''
\end{answer}

\begin{review}
  It is not always obvious that Scalability could be badly affected in
  a Tight Integration. The example of a liferay portlet is actually
  potentially demonstrating the opposite. Portlets can be independent
  web application that can be even remotely deployed (via WSRP). The
  integration of a workflow engine via ad-hoc portlet applications
  could instead guarantee quite of a level of decoupling from the rest
  of the gateway. Here, the limitations highlighted seems to be a bit
  superficial.
\end{review}

\begin{answer}
  We agree with the reviewer that using WSRP (Web Services for Remote
  Portlets) changes the architecture. When portlets are deployed on
  different hosts, we think that the architecture becomes service
  invocation as the portlets then behave as individual software
  components according to our definitions. We added the following comment as a footnote in the presentation of tight integration in section 2.2:
\revised{Note that the
    particular case of portlets can be a bit ambiguous: when portlets
    are deployed remotely, using a protocol such as Web Services for
    Remote Portlets (WSRP), they become independent software
    components according to our definition because they could be
    operated by different teams on different hardware systems. In this
    case, the system should be classified in the service invocation
    architecture since the portlets actually become Web
    Services. The same comment holds for remote object
    invocation.}
\end{answer}

\begin{review}
  The "Specific Feature", referring to the Fine-grained debugging does
  not specify what is relevant to be monitored and who is going to
  gain benefit from the monitoring. The scientists, the gateway
  developers or the administrators? What is a white box, the workflow
  engine or the workflow task? Why multiple software layers affect the
  accuracy of the tasks' internals?
\end{review}

\begin{answer}
  We call ``white-box'' workflow a workflow
  where detailed information about individual tasks is available,
  for instance task statuses, execution logs and resource
  consumption. On the contrary, a ``black-box'' workflow only
  displays global information such as the workflow status and the
  logs of the workflow engine. Fine-grained information about
  workflow tasks helps workflow users (scientists)
  identify errors, it allows gateway administrators to troubleshoot
  executions, and it facilitates debugging by workflow
  developers.
  
  Multiple software layers affect the accuracy of the information
  reported about the tasks' internals because each layer potentially
  shields or modifies information. Task ids, for instance, may not
  be properly linked to workflow activities if delivered through a
  software layer that hasn't been instrumented for that. \todo{refer
    to olabarriaga2014}.
  
  The paragraph on fine-grained debugging has been edited accordingly, at the end of Section 3.
\end{answer}

\begin{review}
  To conclude, the general feeling is that the definition of the
  metrics and the interaction patterns are not rigorous enough to
  provide an interpretation of the final evaluation which is entirely
  useful for the reader.
\end{review}

\begin{answer}
  We hope that the explanations provided in this letter and the
  updated text in the paper demonstrate that the metrics and
  definitions are rigorous.
\end{answer}

\begin{review}
  Probably, some more information on how "Future Generation" systems
  should be designed, starting from the general overview provided,
  with more technical understanding of the system complexities, could
  represent a more valuable contribution to the main aim of this
  particular publication.
\end{review}

\begin{answer}
  We added a section in the paper (current section 6) to show how our
  evaluation framework could be used in the design of new systems.
\end{answer}

\begin{review}
  To improve the reading, Dedicating separate sections to the
  introduction of the metrics and the evaluation of each pattern would
  be beneficial. For instance Section 3 Metrics, Section 4 Evaluation.
\end{review}

\begin{answer}
  This was done as suggested.
\end{answer}

\section{Reviewer \#2}

\begin{review}
  This work describes and evaluates six software architectures
  commonly used to integrate workflow engines into science gateways
  with metrics for assessment of integration complexity, robustness,
  extensibility, scalability and support for meta-workflows and
  fine-grained debugging. The paper is well written in general, and
  the motivation is very nice, clearly comparing the different types
  of architectures that can be found at the back end of a science
  gateway. However, the work has a list of issues that could be
  addressed by the authors:

1. Apache Araviate and WS-PGRADE/gUSE are not science gateways. They
are open source science gateway framework that enables users to access
computing infrastructures frameworks. Therefore, in the Table 1, where
the classifications of science gateways are displayed, it should list
the science gateways that have used those architectures instead of the
frameworks. Otherwise, is quite confusing.
\end{review}

\begin{answer}
We thank the reviewer for the thorough review and useful comments.

We totally agree with the reviewer that Apache Airavata and
WS-PGrade/gUSE are gateway frameworks. This is how they are presented
in the text. Instead of providing an incomplete list of gateways that
use these frameworks in Table 1, we now refer explicitly to ``gateways
based on Apache Airvata'', ``gateways based on WS-PGRADE/gUSE'' and
``gateways based on the Catania Science Gateway
Framework''. Consistently, the table doesn't mention the individual
gateways based on these frameworks any more.
\end{answer}


\begin{review}
  2. Tight Architecture -→ What happens if the workflow engine is
  built into the Science Gateway, but still need a submission
  framework (e.g. WS-PGRADE/gUSE or SAGA) for submitting the enteir
  workflow to the infrastructure.  It would be a tight architecture or
  a task architecture ? I can not see this type of architecture
  reflected in the paper.
\end{review}

\begin{answer}
  The task submission framework is part of the infrastructure
  and its nature doesn't influence the architecture used to integrate
  workflow engines in the science gateway. Formally, it is part of
  interaction \texttt{b} (see definition in 2.1).

  If the ``entire workflow'', i.e. the complete control and data flow,
  is submitted to the infrastructure through interaction \texttt{b} as
  a single task that will manage the execution and potentially submit
  sub-tasks, then this is a task encapsulation architecture. In
  particular, a dedicated engine instance can easily be started for
  every workflow execution.

  If only the workflow tasks are submitted to the infrastructure
  through interaction \texttt{b}, and the main workflow logic is
  controlled from within the science gateway, then this is clearly a
  tight integration.

  The reviewer's use case, however, suggest that the workflow engine
  is split in two parts: (1) a high-level part integrated in the
  science gateway, which submits ``entire workflows'' to the
  infrastructure; (2) a lower-level part, executed on the
  infrastructure, which executes such ``entire workflows''. It seems
  to us that this matches the definition of a nested workflow model,
  where the parent engine is integrated in the science gateway, and
  the child engine runs on the infrastructure. This is another
  instantiation of the nested workflow abstract model, which is not
  evaluated in the paper as mentioned in section 5.2: "\emph{The
    abstract nested workflows and workflow conversion patterns were
    instantiated with service invocation so that they can be analyzed
    in the same framework as the other architectures. Other types of
    instantiation, for instance nested workflows with task
    encapsulation, could also be envisaged.}". It sounds like the
  architecture described by the reviewer is a nested workflow pattern
  instantiated with a tight integration (parent engine) and task
  encapsulation (child engine).

  We found this scenario interesting and we evaluated it in our
  framework. The analysis is reported in the new section 6 of the
  paper.
\end{answer}

\begin{review}
  3. Why there isn't a figure to illustrate an example of a real
  science gateway that corresponds to this architecture, like the
  other types of archtictures (e.g. Figure 3, Figure 4)
\end{review}

\begin{review}
  4. In my opininion Figures 3, 4, 5, 6, should use the iteractions
  explained in the subction 2.1, instead the steps/calls, to be more
  clear. Right now, they are difficult to read.
\end{review}

\begin{review}
  5.  Subsection 2.4, "see for instrance attribute depend of option -W
  torque"-→ repharase, and explain it better.
\end{review}

\begin{review}
6.  Figures 4, 5, 6 are very difficult to understand. Maybe will be a
good idea to create a subsection for explaining them better and reduce
the captions of those figures.
\end{review}

\begin{answer}
  As suggested by the reviewer, the presentation of these Figures has
  been gathered in a separate section which is now the new section 3.
\end{answer}

\begin{review}
  7.  Metrics: The metrics that have been used in this paper favour
  the simplest architectures. For example, looking at the Robustnes
  metrics, R1 and R2, they measure the probability that a workflow
  fails due to the errors of components and interactions that contains
  each architecture. But those metrics do not measure how "grave" are
  those failures and how "difficult" are those failures to repair. The
  same thing for the "Extensibility" metrics, some components can be
  much more difficult to "replace" than others.  Maybe would be a good
  idea to ponderete those metrics based on the number of components or
  interactions (e.g E.1---> dividing the number of interactions or
  components that need to be modified to integrate a new workflow
  between the total number of interactions or components. )
\end{review}

\begin{answer}
  We agree with the reviewer that different weights could be given to
  interactions and software components depending on the ``gravity'' of
  the associated failures, the ``difficulty'' to repair them and even
  the difficulty to implement such interactions and components in
  practice (for instance, interaction \texttt{g} is clearly very
  difficult to implement).

  However, we believe that assigning such weights is specific to the
  case of a particular system, i.e. a science gateway and workflow
  engine(s). Very different ponderations could be obtained depending
  on the technical context, for instance implementation language, type
  of targeted infrastructure (dedicated cluster vs. grid vs. cloud)
  and characteristics of the workflow engine(s) and language(s).

  Instead, we shared a spreadsheet at
  \url{https://frama.link/LrPRsCQp} that allows editing the following
  weights (see also answer to Reviewer \#1):
  \begin{itemize}
  \item Weight given to software components and interactions in
  ``integration'', ``extensibility'' and ``robustness'' criteria.
  \item Weight given to metrics in a criterion (e.g., E$_1$ vs. E$_2$ in
  ``extensibility'').
  \item Weight given to criteria in overall evaluation of Table 3.
\end{itemize}
The paper references this spreadsheet wherever equal weights are
assumed. The readers and reviewers are invited to rely on this
spreadsheet for any issue related to the importance given to a
specific component, interaction, metric or criterion in the
analysis. In the paper, we assumed that all weights equal 1, which we
believe is a reasonable choice although inherently limited.

We find the particular ponderation method suggested by the reviewer
(normalization by the number of components or interaction in the
architecture) a bit unfortunate because it would give different
ponderations to the same component or interaction depending on the
architecture in which it is used, which we think would obfuscate the
comparison.
\end{answer}

\begin{review}
8. In my opinion, if the workflow it is integrated in the science
gateway (tight architecture), then the science gateway should be
evaluated as a component, since it also could fail. The same thing for
the task encapsulation and the infrastructure, in that case, the
Infraestructure should be also considered as component. So, I would
modifiy the table 2, according to that ( e.g., R1=1 for Tight and Task
architecture).
\end{review}

\begin{answer}
  We believe that the robustness of a system for a particular function
  should be evaluated w.r.t the set of components and interactions
  that are \emph{specific} to this function. Indeed, the failure of
  non-specific components is likely to impact many other functions
  that are required by the particular function of interest. To take an
  extreme example, a failure of the internet connection of the gateway
  user would certainly prevent him/her to launch a new workflow, but
  it does not really make sense to integrate the internet connection
  in the robustness analysis because it is not specific to workflow
  execution. Similarly, the failure of the science gateway would
  prevent users to login in the first place, which would prevent
  workflow execution in any case. Likewise, the failure of the
  infrastructure would make any workflow execution worthless in any
  architecture because infrastructure is used for other critical
  functions such as task execution and data storage.

  For these reasons, we disagree with the suggestion made by the
  reviewer. In any case, science gateway and infrastructure are
  present in all the architectures, which would only add 2 to all the
  values of R1.
\end{answer}

\begin{review}
9. According with Figure 2.I, this workflow architecture has 3
interactions b, c1 and c2, since data movement can be done by the
users and workflows in this type of architecture (those interactions
are not exlcuyents) so R2=3
\end{review}

\begin{answer}
  In Figure 2.I, interaction \texttt{c$_1$} is not specific to
  workflow execution since it is used to transfer any data to the
  infrastructure. This is why we don't count it in R2, for the same
  reason as explained previously.
\end{answer}

\begin{review}
  10. The text of Sections 3.2, 3.3, 3.4, 3.5, 3.6 and 3.7 could be
  reduced, since most of the values are obvious from Figures 2.II and
  Table 2.
\end{review}

\begin{answer}
  While we agree that the Sub-Sections in Section 3 look a bit
  repetitive, we believe that the metrics evaluation reported in Table 2 needs to be
  properly justified. 
\end{answer}

\begin{review}
  11. Service invocation and Task encapsulation can also be the same
  thing
\end{review}

\begin{answer}
  Service invocation and Task encapsulation are fundamentally different for the following reasons:
  \begin{itemize}
  \item In task encapsulation, the workflow engine runs on the infrastructure while in service invocation, it is hosted by a dedicated service.
  \item In task encapsulation, the workflow engine is executed as a task of the infrastructure while in service invocation, it is invoked through a service call. That means, in particular, that in task encapsulation different workflow engines are executed through the same protocol (a task), while different types of service calls might be involved in service invocation (e.g. REST or SOAP Web-Service).
  \item In task encapsulation, the workflow engine does not transfer data to the infrastructure while in service invocation it does (interactioni \texttt{c$_2$}.
  \item In task encapuslation, the science gateway submits tasks to the infrastructure, while in service invocation the workflow engine does.
  \end{itemize}
  Therefore, while we acknowledge that task encapsulation and service
  invocation share characteristics, we believe that their differences
  can be clearly inferred from section 2 of the paper. 
\end{answer}

\begin{review}
12. No so sure that tight integration are the most easy for
integrating a workflow engine, since in case a the workflow engine
needs to be modified, the whole science gateway has to be change
(e.g. re-programe a portlet in Liferay). On the other hand, with
Service invocation, the replacement could be very much more easier.
\end{review}

\begin{answer}
  We assume that the reviewer is referring to the I$_1$ metric which
  is, indeed, lower for the tight integration than for any other
  architecture (it is equal for task encapsulation). We agree with the
  reviewer that the difficulty to update a component depends on the
  nature of this component. However, we believe that a quantification
  of this difficulty is very much dependent on the particular system
  under study. For instance, in some cases, tightly integrating a
  workflow engine may be straightforward (e.g. when the science
  gateway has the appropriates hooks) while developing a service may
  require a lot of work if it has to be done from scratch.

  We believe that this discussion comes down to the assignments of
  different weights to the interactions and components in the
  architectures. As discussed before, these weights can now be
  adjusted using the spreadsheet at \url{https://frama.link/LrPRsCQp}
  to better describe a particular system.
\end{answer}

\section{Reviewer \#3}

\begin{review}
  This paper provides a comparison of 6 different software
  architectures commonly used to integrate workflow engines into
  science gateways. For this comparison, the authors define seven
  different types of interactions to outline the differences in
  approach. The combination of components and interactions for the 6
  overview models in Figure 2 that form the underlying basis for the
  comparisons.

  The authors then provide a set of Metrics: complexity, robustness,
  extensibility, scalability \& specific features to compare and
  contrast the different software architectures.

  Complexity, for example, is measured by counting the number of
  components and interactions that were defined and annotated in
  Figure 2.  Extensibility measures the difficulty to replace or add
  elements to the architecture e.g. a workflow engine, an engine
  version update, add a new workflow or add new infrastructure.

  I liked this paper but I also struggled a lot with it too because
  ultimately, whilst this metric structure provides some rudimentary
  structure to quantify the various software architectures, a many
  software architecture perspectives are missing, and the results are
  not normalized against the functionality a system provides.

  For example, when you normally think of software architectures, a
  good design is often one that is layered and componentized. There
  are multiple benefits of such an an approach over monolithic systems
  but using the metrics defined in this paper, good software practices
  to expose functionality is not really factored in. So, the result is
  that the more layers to components in the system, the worse it
  scores. So, these aspects of good software engineering or providing
  enhanced functionality seem penalized in this paper, as it stands.
\end{review}

\begin{answer}
We thank the reviewer for the thorough review and useful comments.

Regarding componentization: our architectures are described in terms
of software components and interactions among these components (see
Figure 1). So we believe that our approach fits in the context of
component-based software engineering. The problem that we address is
to determine what components should be defined and how they should
interact to optimize the metrics. For instance, the comparison between
the tightly integrated and service invocation architectures evaluates
whether the workflow engine should be a specific component or if it
should be integrated in the science gateway. Besides, the reviewer's
comment touches upon the granularity at which components are
defined. We extended the first paragraph of Section 2 to address this issue:
\revised{Software
  components are defined at a granularity such that two components may
  potentially be operated by different teams on different hardware
  systems. For instance, the science gateway and infrastructure are
  usually distinct components. In real systems, software components
  may exist at a finer granularity, to implement functions such as
  authentication, data persistency or logging. Such fine-grained
  components are not covered by our analysis since we focus on the
  interactions between the science gateway, the infrastructure and the
  workflow engine.}

The discussion on software layers is more difficult. Roughly, our
architectures could be described as multilayered software
architectures. For instance, the tight integration has two layers
(science gateway and infrastructure), the service invocation has
three, the task encapsulation has two, etc. However, strictly
speaking, these are not layers because it would mean that layer n-1
can only communicate with layer n, which is not the case. For
instance, in service invocation, layer 0 (science gateway)
communicates with layer 2 (infrastructure) to transfer data. And in
task encapsulation, layer 1 (infrastructure) communicates with layer 0
(science gateway). Thus, we don't think that referring to multilayered
architectures is accurate here. 

We don't think that our architectures lead to conclude systematically
that ``the more layers to components in the system, the worse it
scores'', as suggested by the reviewer. For instance, service
invocation scores better than tight integration in E1 and S1 due to
the additional ``layer'' used to host the workflow engine. Likewise,
nested workflows score better than tight integration for S2. However,
our metrics do imply that more layers leads to more integration
complexity and reduced robustness, which we think is very commonly
experienced in distributed systems, where  communication between
``layers'' involve wide-area network communications between machines
that may be operated by different people. We added a note in the definition of our robustness metrics to reflect this discussion:
\begin{quote}
 Robustness is measured here as a consequence of global
complexity, since complex architectures tend to be more prone to
failure, \revised{in particular in distributed systems where
  interactions between components involve wide-area network
  communications between machines that may be operated by different
  people}.
\end{quote}
\end{answer}

\begin{review}
An example where the logic fails I believe is in the Extensibility. Let's take two cases initially:
-	Pool architecture
-	Nested.
For pool this score is 9 but for nested the score is 12.5. So, using these metrics the pool architecture is easier to extend.  But, if this is based on the same interface (e.g. SHIWA), then the difficulty to extend each system seems quite similar, because basically a workflow engine needs to implement the SHIWA course grained ORE interface to allow it to work. If it implements this interface as a client and a server then it can both consume other workflows and be embedded by them. For the pool architecture in the most generic sense, a workflow would need to do both (to digest ORE bundles and to pass them on), and for a nested architecture, a workflow engine would need to do one of the other depending on whether it was calling another workflow or being invoked by one.  However, as I understand, the case here assumes there is no connectivity (passing of bundles) in the pool case, so it seems less complex to extend. So, in the end, it is not about how complex the extension is
it is about what you want to do.  For example, for the nested case, you could always assume one master workflow, then it would be simpler (but less flexible). So in the end this table could look very different depending on how the different systems are defined in scope, and you could potentially have various versions of either of these cases. 
\end{review}

\begin{answer}
  The issue raised by the reviewer needs to be discussed metric by
  metric rather than on the global extensibility score. The pool and
  nested architectures differ in metrics E$_1$ (adding a new engine
  type) and E$_4$ (adding a new infrastructure). For E$_4$ (adding a
  new infrastructure), it seems clear to us that the nested
  architecture is more difficult to extend because it has two workflow
  engines while the pool has only one. Therefore, interactions
  \texttt{b} and \texttt{c$_2$} have to be counted twice, which
  explains the difference in scores for E$_4$. 

  The situation highlighted by the reviewer is related to E$_1$ (new
  engine type). If a new engine is added to the pool model, the
  following interactions and components have to be updated:
 \begin{enumerate}
 \item The engine has to be wrapped in the SHIWA Agent, which includes
   the reading/writing of ORE bundles so that the agent can get/pass
   them to the pool through interaction \texttt{f}. In practice, the
   agent component has to be modified to convert ORE bundles to the
   input format supported by the engine, and to assemble ORE bundles
   from the output format produced by the engine. As suggested by the
   reviewer, another implementation possibility would be to modify the
   engine to read/write ORE bundles so that the integration in the
   agent is straightforward.
 \item The engine has to be able to transfer data to the
   infrastructure used in the science gateway, i.e. interaction
   \texttt{c$_2$} has to be implemented in the engine.
 \item The engine has to be able to submit tasks to the infrastructure
   used in the science gateway, i.e. interaction \texttt{b} has to be
   implemented in the engine.
 \end{enumerate}
This is why E$_1$=3 for the pool model.

If a new child engine is added to the nested model then the following interactions an and components have to be updated:
\begin{enumerate}
\item A workflow service has to be created for the workflow engine.
\item The parent engine has to be interfaced with the new workflow
  service, i.e., the workflow service should be able to digest ORE
  bundles, or the parent engine should convert ORE bundles to the
  format supported by the workflow service. ORE bundles will be
  exchanged through interaction \texttt{d}.
 \item The child engine has to be able to transfer data to the
   infrastructure used in the science gateway, i.e. interaction
   \texttt{c$_2$} has to be implemented.
 \item The child engine has to be able to submit tasks to the infrastructure
   used in the science gateway, i.e. interaction \texttt{b} has to be
   implemented,
\end{enumerate}
This is why E$_4$=4 for the nested model.

The situation described by the reviewer focuses on item 1 of the pool
extension and item 1 and 2 of the nested workflow extension
(infrastructure is not a discriminant factor here). First, the
reviewer suggest that the availability of ORE bundles in the SHIWA
framework standardizes the communication with workflow engines so that
it becomes possible to have a generic workflow service that can
consume and produce ORE bundles regardless of the particular engine(s)
it wraps. So indeed, item 1 of the nested case disappears. This is a
typical case where, as we wrote in section 7.2, \emph{The particular
technical or historical context of a science gateway project may also
influence the \revised{evaluation} of an architecture.} In this
case, the possible availability of a generic workflow service based on
ORE bundles is contextual information that can hardly be integrated in
the generic analysis. We believe that our framework helps evaluate
the impact of technologies such as ORE bundles on architectures, as
discussed here.

Second, the
reviewer suggests that the integration of the workflow engine in the
agent (pool model) is easier than in the service because the engine
has to produce and consume ORE bundles in the former case while it
only has to consume them in the latter case. This is true again, but
it comes down to the discussion on the weights of the different
software components. In this particular case, it would make sense to
assign a higher weight to the agent than to the service, which could
be done through the public spreadsheet.  
\end{answer}

\begin{review}
  Adding functionality e.g. in this case to add the ability to pass
  bundles to one system and another is a positive (and complex) thing
  to accomplish. There should be a way of counting this and having
  this score higher than Tight, Service or task, or normalizing this
  functionality into the metrics to provide a more balanced view. The
  "Scalability" touches on this but "Functionality" could be another
  aspect here to consider.
\end{review}

\begin{answer}
  We agree with the reviewer that functionality is important. This was
  exactly the goal of the ``Specific features'' evaluation
  criterion. We renamed this criterion ``Functionality'' as we find it
  a much better name indeed. We also renamed O$_1$ to F$_1$ and O$_2$
  to F$_2$. 

  However, we don't think that ``the ability to pass bundles'' should
  be listed as a metric in the functionality criterion, since this
  feature remains quite specific to the particular context mentioned
  by the reviewer. 
\end{answer}

\begin{review}
  Another example is for complexity.  Again, the less components and
  interactions the system has the less complex it is. But in practice,
  it is not so simple. For example, the tight integration is where the
  workflow system is integration directly into the science gateway and
  into the infrastructure. Whereas this architecture has the least
  number of components and interactions of them all it does not mean
  it is less complex in practice. Integrating a workflow into science
  gateway and directly on top of the infrastructure depends very much
  on the tools they sit upon and the complexity of that
  integration. Such tight integrated systems therefore could be
  massive monolithic pieces of code that are extremely difficult and
  complex to develop and maintain.  Whereas a task-based approach or
  service based approach through breaking code out to different
  services and tasks may be far simpler to develop and maintain.
\end{review}

\begin{answer}
  We totally agree with the reviewer. However, we think that this
  issue cannot be addressed at the pure architectural level as it
  requires accurate technical information about systems. In some
  systems, the science gateway may become a massive piece of
  unmaintainable code as described by the reviewer, and in some other
  cases, it may have a neat internal architecture that allows for easy
  integration of workflow engines. Once again, we believe that the
  answer to this issue is about assigning weights to the different
  components and interactions, which is now possible through the
  public spreadsheet. For instance, if the science gateway code is
  known to be ``untouchable'' due to a cumbersome, then a high weight
  should be put on the science gateway component. 
\end{answer}

\begin{review}
  I therefore think that the metrics defined only provide a very high
  level structure for an evaluation of the different software
  architectures and they should discuss other factors and scenarios
  that can affect development. 
\end{review}

\begin{answer}
  The metrics provide an abstract framework for evaluating
  architectures independently from any particular system. The goal is
  to abstract architectural discussions from the particular technical
  context of a real system. Some degree of system-specificity can now
  be introduced by assigning weights to interactions and components
  through our public spreadsheet, which we think provides a way to apply our
  analysis to study real systems more precisely.
\end{answer}

\begin{review}
Some of these limitations are discussed in section 4.2 but I think an
extra section discuss these before the definition of the metrics would
be very helpful to set the scene correctly for the
evaluation.  Otherwise, the reader is left asking a number of questions
during the evaluation section. 
\end{review}

\begin{answer}
We added the following paragraphs to the introduction to section 3:

\revised{Note that the metrics defined hereafter aim at providing a framework
to evaluate architectures independently from any particular system,
regardless of any technical constraint such as the use of a specific
programming language or the internal architecture of the science
gateway. The goal is to abstract architectural discussions from the
particular technical context of a real system. Some degree of
system-specificity can be introduced by assigning weights to
interactions and components through a public spreadsheet available at
\url{https://frama.link/LrPRsCQp}. In particular, sheet "Components
and interactions" contains the weight given to these entities in the
metrics and can be used to produce new versions of
Table 2 that fit the particular technical context
of a real system. For instance, a high weight could be assigned to the
science gateway if its internal architecture is such that it can
hardly be extended in practice.}

\revised{In addition, all the metrics in a criterion contribute
  equally to the global score used for the criterion. Column B of
  sheet ``Metrics'' of the public spreadsheet may be used to assign
  different weights to each metric to adjust the specific context of a
  study.}
\end{answer}

\begin{review}
I also think that a few more metrics
e.g. "functionality" could be added to provide a more balanced view of
the different approaches.

 Another useful extension of the paper could
be to normalize the results against the functionality each approach
provides (perhaps in a separate table) - this may help to provide a
more balanced table that factors in functionality.
\end{review}

\begin{answer}
  As explained previously, the last evaluation criterion, now named
  ``Functionality'' aims at this purpose. We hope it clarifies that
  functionality is actually part of the analysis. 
\end{answer}

\end{document}