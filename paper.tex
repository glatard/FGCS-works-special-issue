\documentclass[preprint,3p,twocolumn]{elsarticle}

\usepackage{svg}

\newcommand{\todo}[1]{\color{blue} #1 \color{black}}
\newcommand{\note}[1]{\color{blue}\textbf{Note: }\textit{#1} \color{black}}

\begin{document}
\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{Architectures for workflow integration in science gateways}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \address[label1]{}
%% \address[label2]{}

\author{}

\address{}

\begin{abstract}
%% Text of abstract

\end{abstract}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%\authors{Tristan Glatard, Marc-Etienne Rousseau, Pierre Rioux, Samir
%Das, Natacha Beck, Reza Adalat, Pierre Bellec, Pierre-Olivier Quirion,
%S\'ilvia D. Olabarriaga, Sorina Camarasu-Pop, Alan C. Evans}
% Rafael? Ewa? Peter Kacsuk? Johan? Satra? Najma?

\journal{Future Generation Computer Systems}

\maketitle

\section{Introduction}

\paragraph{Context} The question of integrating workflow engines in
science gateways can be seen at various levels, corresponding to
various definitions of workflows. One level is the SHIWA level, where
it was considered that workflow engines are aware of the DCI (and 'D'
is important because of data transfers, proxies, etc). Another level
is to remove 'D' from the definition: a workflow engine becomes a
program that submits jobs, potentially only to local clusters. It
opens a whole new class of workflow engines that we used to consider
as "applications". For instance, in neuroinformatics: Nipype, PSOM,
but also FSL through the fslsub tool. A workflow engine is not
supposed to be aware of the science gateway.  A wide-array of workflow
engines are available with specificies coming from the application
domain, available tools, etc Their integration in science gateways
becomes critical. Several of the examples presented in the
paper will be taken from medical image analysis, in particular
neuroimaging.

\paragraph{Goal} this paper reviews and compares the architectures to
integrate workflow engines in science gateways.

\paragraph{Contributions}
\begin{itemize}
\item We evaluate architectures based on our experience with existing systems
\item We propose a new architecture
\end{itemize}

\section{Workflow engines and science gateways}

\subsection{Workflow engines}

In the last decade, the e-Science workflow community has developed
high-level workflow systems to help developers access distributed
infrastructures such as grids and web services, resulting in tools
such as Askalon, Hyperflow, MOTEUR, Pegasus, Swift, Taverna, Triana,
VizTrails, WS-PGRADE/gUSE, etc. Such workflow engines usually describe
applications in a specific language that offers operators for parallel
computing, visual description and edition, links with domain-specific
tool repositories, etc. Descriptions and experiments conducted with
e-Science workflow engines were published in journals such as Future
Generation Computer Systems and conference venues such as
WORKS. 

At the same time, specific toolboxes were emerging in various
scientific domains to facilitate interactions between different
software components. In neuroimaging for instance, tools such as
Nipype, PSOM, SPM and FSL provide abstractions and functions to define
processes that handle the data flow between other processes. Soon,
such tools were interfaced to computing systems, in particular
clusters: they were extended to create cluster tasks, handle their
dependencies, and execute them on clusters. For instance, FSL can
launch tasks on SGE through its \texttt{fsl\_sub} tool, Nipype
\todo{...}, PSOM \todo{...}, and SPM \todo{...}. Although distributed
infrastructures are hardly handled, such domain-specific tools have to
be called workflow engines. They represent a tremendous opportunity
for science gateways to leverage existing tools and
applications. Whether these should be combined with e-Science workflow
engines is an open question.

A workflow engine is defined as a software that submits jobs to a
cluster, grid or cloud. Workflows may be expressed in any language,
including scripts. Several workflow engines may be combined in the
same science gateway. Workflow: a process that submits tasks to the
infrastructure. FSL tools, say Feat, can be considered as workflows
when they are configured to submit tasks to clusters using fslsub. A
workflow consists of activities. We are talking only about concrete
workflows with a configuration (see terstyansky et al 2014 "enabling
scientific workflow sharing...")

\subsection{Science gateways}

Science Gateway: An ecosystem, usually accessible through a web
  portal, that provides tools to access distributed
  infrastructures. Tools usually help users manage data transfers,
  task execution and authentication on multiple computing and storage
  locations. Examples: CBRAIN, VIP, NSG, etc


\subsection{Infrastructures}

\todo{Maybe this is not relevant}

\section{Architectures}

We define architectures from their main components and the interactions
between these components.

A workflow engine is not supposed to be aware of the science gateway (?).

\subsection{Common components}

Infrastructure: computing and storage resources. Includes
  schedulers, catalogues, etc. Can be grid, cloud or even
  cluster. Some workflow engines may assume a shared file system
  usually available only on clusters.

  Workflow engine: executes workflows, i.e. process their dependencies
  and create computing tasks. Might transfer data too.

\subsection{Interactions}

a. 

d. Sub-tasking: science gateway knows the sub-tasks of a
task. Therefore, it can kill all sub-tasks of a task when a task is killed.

g. Workflow integration results in an interface, for instance a web form,
where users can enter the parameters of the workflow to be
executed. Integrating a workflow is not the same process as
integrating a workflow engine. Integrating a workflow engine results
in an interface that allows developers to enter the parameters of the
workflow engine.


Only workflow execution is described. 
Workflow integration is not covered. 

\subsection{Tight integration}

\begin{figure}
\centering
\def\svgwidth{0.5\columnwidth}
\input{images/tight.pdf_tex}
\label{archi:tight}
\caption{Tight integration}
\end{figure}

\subsection{Service invocation}

Or as a component.

= Examples
* VIP
* Silvia's gateway, starting from VBrowser

P-Grade: http://onlinelibrary.wiley.com/doi/10.1002/cpe.1654/full

= Description 
*  Workflow engine is deployed as a (Web) service called by the
science gateway. Probably the most adopted architecture. 

= Evaluation
+ Easy to implement in the SG (just call the service)
- Load-balancing between services is difficult
(refer to VIP's experience).
- Need to wrap workflow engines in
services.
- Need to know where to submit each workflow, e.g. when
multiple workflows are involved.
- The science gateway has no
knowledge of the detailed progress of the workflow, in particular
job-level information (statuses, stdout, stderr). If we wanted to do
that, specific feedback channels have to be developed, which requires more work (see VIP).

\begin{figure}
\centering
\def\svgwidth{0.5\columnwidth}
\input{images/service.pdf_tex}
\label{archi:service}
\caption{Service integration}
\end{figure}


\subsection{Sub-tasking}

Examples:
* CBRAIN and Niak, CBRAIN and FSL, CBRAIN and Nipype if time permits

+ Simplified parallelization
- Needs extension on the workflow engine side. Simple if workflow engine already supports task submission.

In  this model, the task of the SG is only to submit command line tools and command line tools submitted by these command line tools. The architecture is really upsided-down (was: workflow engine on top, jobs below. Is: job on top, workflow engine below). And this is what it makes it so powerful.

Sub-tasking also requires dependencies between tasks.

Can support various models such as pilot jobs, fsl sub, etc.

No global vision of the workflow. 

Science gateway has to make sure that the machine on which a workflow
task executes is reliable as several sub-tasks will depend on the
success of this task. Or workflow engine task has to be recoverable.

Performance: there could be a deadlock when there is contention. Also,
the workflow task has to be reliable (for instance, proper walltime
estimation), which is difficult because these tasks are
long. Restarting a workflow task may mean that all the tasks in the
workflow have to be restarted, depending on the capability of the
workflow engine. This complexifies task scheduling (a system could
ignore these issues but they may backfire).


\subsection{Agent model}

= Examples
* SHIWA Pool

= Evaluation
+ Scale-up and load balancing
+ Multi-language
- Needs a complete framework (the pool). Most likely third-party software as it's a lot of work to implement.

\subsection{Nested workflows}

=  Examples
* SHIWA CGI, but SHIWA CGI is more complex because it enables meta workflows. In general, if we wanted only to use an embedded workflow, we don't need the meta-workflow for that. 
* Maybe other tool integrations in CBRAIN or VIP. For instance FSL or any tool that can be parallelized by this is not exploited.

Refer to the recent paper from Kacsuk. 

Command line tool can run anwywhere.
- Parallelization is tricky because command line tool can run anywhere, on any cluster. Sometimes, it is solved by wrapping workflow engine in another worfklow engine, to iterate, e.g. on subjects. That's what we do in VIP. But this is not ideal.
+ No extension required on the SG side to support a workflow engine, as long as it can wrap command line tools.

\subsection{Workflow import}

= Examples
* IWIR (SHIWA FGI). Language conversion.
* first Niak+CBRAIN integration?

= Evaluation
* To interpret the language, you need either a generic interpreter or a conversion tool. Quite complex and hard to  maintain.

\section{Evaluation}

\subsection{Criteria}

Complexity of the architecture (therefore development effort, robustness, maintainability, etc):
  * On the SG side
  * On the workflow engine side (what is required to wrap it).
Could be measured by counting the number of required functions,
e.g. submit job, call a workflow service, etc. Ponderated by the
"complexity" of the features between 1 (easy) and 3 (tricky). Look at
other types of measures.

Provided features:
  * Fine-grained workflow monitoring (debugging, progress report)
  * Scalability, load-balancing between engines
  * Support for multiple engines in the same gateway. Allows to reuse several different workflows in a single gateway but less complex than meta workflows. But do we really need meta workflows?
  * Support for meta workflows

"A Formal Approach to Support Interoperability in Scientific
Meta-workflows" (reviewed for IWSG and JoGC) has a formal model to
evaluate CGI and FGI.

"four major approaches for workflow interoperability include a,b,c,d" (see Terstyanzky et al, "Enabling scientific workflow sharing through CGI...", FGCS 2014) -> read this ref.

http://www.oreilly.com/programming/free/files/software-architecture-patterns.pdf

\subsection{Evaluation}

\section{Discussion}

These architectures can be combined (give examples). 

\section{Conclusion}

\section{Acknowledgments}

FLI-IAM, Labex PRIMES, Ludmer Centre

\bibliographystyle{elsarticle-num} 
\bibliography{biblio}

\end{document}
\endinput
