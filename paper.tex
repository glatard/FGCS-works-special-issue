\documentclass[preprint,3p,twocolumn]{elsarticle}

\usepackage{svg}
\usepackage{url}
\usepackage{xspace}

\newcommand{\todo}[1]{\color{blue}\xspace\emph{#1}\xspace\color{black}}
\newcommand{\note}[1]{\color{blue}\textbf{Note: }\textit{#1} \color{black}}

\begin{document}
\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{Architectures for workflow integration in science gateways}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \address[label1]{}
%% \address[label2]{}

\author{}

\address{}

\begin{abstract}
%% Text of abstract

\end{abstract}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%\authors{Tristan Glatard, Marc-Etienne Rousseau, Pierre Rioux, Samir
%Das, Natacha Beck, Reza Adalat, Pierre Bellec, Pierre-Olivier Quirion,
%S\'ilvia D. Olabarriaga, Sorina Camarasu-Pop, Alan C. Evans}
% Rafael? Ewa? Peter Kacsuk? Johan? Satra? Najma?

\journal{Future Generation Computer Systems}

\maketitle

\section{Introduction}

\paragraph{Context} The question of integrating workflow engines in
science gateways can be seen at various levels, corresponding to
various definitions of workflows. One level is the SHIWA level, where
it was considered that workflow engines are aware of the DCI (and 'D'
is important because of data transfers, proxies, etc). Another level
is to remove 'D' from the definition: a workflow engine becomes a
program that submits jobs, potentially only to local clusters. It
opens a whole new class of workflow engines that we used to consider
as "applications". For instance, in neuroinformatics: Nipype, PSOM,
but also FSL through the fslsub tool. A workflow engine is not
supposed to be aware of the science gateway.  A wide-array of workflow
engines are available with specificies coming from the application
domain, available tools, etc Their integration in science gateways
becomes critical. Several of the examples presented in the
paper will be taken from medical image analysis, in particular
neuroimaging.

\paragraph{Goal} this paper reviews and compares the architectures to
integrate workflow engines in science gateways.

\paragraph{Contributions}
\begin{itemize}
\item We evaluate architectures based on our experience with existing systems
\item We propose a new architecture
\end{itemize}

\section{Workflow engines and science gateways}

\subsection{Workflow engines}

In the last decade, the e-Science workflow community has developed
high-level workflow systems to help developers access distributed
infrastructures such as grids and web services, resulting in tools
such as Askalon, Hyperflow, MOTEUR, Pegasus, Swift, Taverna, Triana,
VizTrails, WS-PGRADE/gUSE, etc. Such workflow engines usually describe
applications in a specific language that offers operators for parallel
computing, visual description and edition, links with domain-specific
tool repositories, etc. Descriptions and experiments conducted with
e-Science workflow engines were published in journals such as Future
Generation Computer Systems and conference venues such as
WORKS. 

At the same time, specific toolboxes were emerging in various
scientific domains to facilitate interactions between different
software components. In neuroimaging for instance, tools such as
Nipype, PSOM, SPM and FSL provide abstractions and functions to define
processes that handle the data flow between other processes. Soon,
such tools were interfaced to computing systems, in particular
clusters: they were extended to create cluster tasks, handle their
dependencies, and execute them on clusters. For instance, FSL can
launch tasks on SGE through its \texttt{fsl\_sub} tool, Nipype
\todo{...}, PSOM \todo{...}, and SPM \todo{...}. Although distributed
infrastructures are hardly handled, such domain-specific tools have to
be called workflow engines. They represent a tremendous opportunity
for science gateways to leverage existing tools and
applications. Whether these should be combined with e-Science workflow
engines is an open question.

A workflow engine is defined as a software that submits jobs to a
cluster, grid or cloud. Workflows may be expressed in any language,
including scripts. Several workflow engines may be combined in the
same science gateway. Workflow: a process that submits tasks to the
infrastructure. FSL tools, say Feat, can be considered as workflows
when they are configured to submit tasks to clusters using fslsub. A
workflow consists of activities. We are talking only about concrete
workflows with a configuration (see terstyansky et al 2014 "enabling
scientific workflow sharing...")

\subsection{Science gateways}

Science Gateway: An ecosystem, usually accessible through a web
  portal, that provides tools to access distributed
  infrastructures. Tools usually help users manage data transfers,
  task execution and authentication on multiple computing and storage
  locations. Examples: CBRAIN, VIP, NSG, etc


\subsection{Infrastructures}

\todo{Maybe this is not relevant}

\section{Architectures}

We define architectures from their main components and the interactions
between these components.

A workflow engine is not supposed to be aware of the science gateway (?).

\subsection{Common components}

Infrastructure: computing and storage resources. Includes
  schedulers, catalogues, etc. Can be grid, cloud or even
  cluster. Some workflow engines may assume a shared file system
  usually available only on clusters.

  Workflow engine: executes workflows, i.e. process their dependencies
  and create computing tasks. Might transfer data too.

\subsection{Interactions}

a. 

c. Data movements can be triggered by the user, to upload input data
or download processed data, or by the workflow engine, to transfer
data across the infrastructure. 

d. Sub-tasking: science gateway knows the sub-tasks of a
task. Therefore, it can kill all sub-tasks of a task when a task is killed.

g. Workflow integration results in an interface, for instance a web form,
where users can enter the parameters of the workflow to be
executed. Integrating a workflow is not the same process as
integrating a workflow engine. Integrating a workflow engine results
in an interface that allows developers to enter the parameters of the
workflow engine.


Only workflow execution is described. 
Workflow integration is not covered. 

\subsection{Tight integration}

See Figure~\ref{archi:tight}. The workflow engine is tightly
integrated with the science gateway, which means that it shares code,
framework components, libraries, etc. For instance, the workflow
engine might be a portlet in a Liferay portal, a controller in a Ruby
on Rails application, and so on. The workflow engine and the science
gateway usually share a database where application, users and other
resources are stored. In this model, task control and data control are
initiated from the science gateway. This is the model adopted in the
Catania Science Gateway Framework~\cite{Ardizzone2012} (see specific
documentation on
workflows\footnote{\url{http://bit.ly/1oQrzvQ}})
and LONI Pipeline Environment~\cite{dinov2009efficient}.\todo{Add better justification}

\begin{figure}
\centering
\def\svgwidth{0.5\columnwidth}
\input{images/tight.pdf_tex}
\caption{Tight integration}
\label{archi:tight}
\end{figure}

\subsection{Service invocation}

See Figure~\ref{archi:service}. The workflow engine is available
externally to the science gateway. The science gateway controls it
through a specific interaction that might be implemented as a
web-service call (e.g. RESTful or SOAP), as a command-line or as any
other method that separates the workflow engine from the science
gateway. The workflow engine might be invoked either as a black box
that completely masks the infrastructure and workflow activities, or
as a grey box that allows for some interaction with them.  The
workflow engine is responsible for controlling the tasks on the
infrastructure, and performing the required data transfers to execute
them. User data is usually managed through the science gateway,
although it might as well be delivered by the workflow engine directly
to the user.

This architecture is largely adopted, in systems such as Apache
Airvata~\cite{marru2011apache}, Vine
Toolkit~\cite{DBLP:journals/scpe/SzejnfeldDKKKKLPTWDNW10}, Virtual
Imaging Platform~\cite{GLAT-13}, the WS-PGRADE/gUSE
framework~\cite{Kacsuk2012} and the numerous science gateway instances
that use it~\cite{kacsuk2014science}.
\begin{figure}
\centering
\def\svgwidth{0.5\columnwidth}
\input{images/service.pdf_tex}
\caption{Service integration}
\label{archi:service}
\end{figure}

\subsection{Sub-tasking}

See Figure~\ref{archi:sub-task}. The science gateway is responsible to
execute the tasks on the infrastructure, dealing with heterogeneous
batch managers and meta-schedulers. The workflow engine is a
particular task that can submit sub-tasks to the science gateway. The
workflow engine keeps track of the dependencies between the sub-tasks
but their execution is delegated to the science gateway. The science
gateway may implement mechanisms to deal with task dependencies such
as basic dependency lists as available in most batch managers. It has
no global vision of the workflow. The fact that tasks are submitted
through the science gateway does not restrict the range of possible
execution models. Pilot jobs, for instance, can still be used.

This model is implemented in CBRAIN~\cite{SHER-14} where it is used to
integrate the PSOM workflow engine and the FSL toolkit through its
\texttt{fsl\_sub} tool. In particular, the CBRAIN-PSOM integration is
described in~\cite{GLAT-16} and uses an agent computing model (a.k.a pilot
jobs).
\begin{figure}
\centering
\def\svgwidth{0.5\columnwidth}
\input{images/sub-task.pdf_tex}
\caption{Sub-tasking}
\label{archi:sub-task}
\end{figure}

\subsection{Pool model}

See Figure~\ref{archi:agent}. Workflows are submitted by the science
gateway to a pool to which agents connect asynchronously to retrieve
and execute workflows (interaction \texttt{f}). Agents may be started according to various
policies, for instance to ensure load balancing. Agents may wrap
different types of workflow engines. This model was implemented in the
SHIWA pool~\cite{ROGE-13}.
\begin{figure}
\centering
\def\svgwidth{0.8\columnwidth}
\input{images/agent.pdf_tex}
\caption{Pool model}
\label{archi:agent}
\end{figure}

\subsection{Nested workflows}

=  Examples
* SHIWA CGI, but SHIWA CGI is more complex because it enables meta workflows. In general, if we wanted only to use an embedded workflow, we don't need the meta-workflow for that. 
* Maybe other tool integrations in CBRAIN or VIP. For instance FSL or any tool that can be parallelized by this is not exploited.

Refer to the recent paper from Kacsuk. 

Command line tool can run anwywhere.
- Parallelization is tricky because command line tool can run anywhere, on any cluster. Sometimes, it is solved by wrapping workflow engine in another worfklow engine, to iterate, e.g. on subjects. That's what we do in VIP. But this is not ideal.
+ No extension required on the SG side to support a workflow engine, as long as it can wrap command line tools.

\subsection{Workflow import}

= Examples
* IWIR (SHIWA FGI). Language conversion.
* first Niak+CBRAIN integration?

= Evaluation
* To interpret the language, you need either a generic interpreter or a conversion tool. Quite complex and hard to  maintain.

\section{Evaluation}

\subsection{Criteria}

Complexity of the architecture (therefore development effort, robustness, maintainability, etc):
  * On the SG side
  * On the workflow engine side (what is required to wrap it).
Could be measured by counting the number of required functions,
e.g. submit job, call a workflow service, etc. Ponderated by the
"complexity" of the features between 1 (easy) and 3 (tricky). Look at
other types of measures.

Provided features:
  * Fine-grained workflow monitoring (debugging, progress report)
  * Scalability, load-balancing between engines
  * Support for multiple engines in the same gateway. Allows to reuse several different workflows in a single gateway but less complex than meta workflows. But do we really need meta workflows?
  * Support for meta workflows

"A Formal Approach to Support Interoperability in Scientific
Meta-workflows" (reviewed for IWSG and JoGC) has a formal model to
evaluate CGI and FGI.

"four major approaches for workflow interoperability include a,b,c,d" (see Terstyanzky et al, "Enabling scientific workflow sharing through CGI...", FGCS 2014) -> read this ref.

http://www.oreilly.com/programming/free/files/software-architecture-patterns.pdf

\subsection{Evaluation}

\paragraph{Service}
+ Easy to implement in the SG (just call the service)
- Load-balancing between services is difficult
(refer to VIP's experience).
- Need to wrap workflow engines in
services.
- Need to know where to submit each workflow, e.g. when
multiple workflows are involved.
- The science gateway has no
knowledge of the detailed progress of the workflow, in particular
job-level information (statuses, stdout, stderr). If we wanted to do
that, specific feedback channels have to be developed, which requires more work (see VIP).

\paragraph{Sub-task}

Science gateway has to make sure that the machine on which a workflow
task executes is reliable as several sub-tasks will depend on the
success of this task. Or workflow engine task has to be recoverable.

Performance: there could be a deadlock when there is contention. Also,
the workflow task has to be reliable (for instance, proper walltime
estimation), which is difficult because these tasks are
long. Restarting a workflow task may mean that all the tasks in the
workflow have to be restarted, depending on the capability of the
workflow engine. This complexifies task scheduling (a system could
ignore these issues but they may backfire).


\paragraph{Pool}

= Evaluation
+ Scale-up and load balancing
+ Multi-language
- Needs a complete framework (the pool). Most likely third-party software as it's a lot of work to implement.


\section{Discussion}

These architectures can be combined (give examples). 

\section{Conclusion}

\section{Acknowledgments}

FLI-IAM, Labex PRIMES, Ludmer Centre

\bibliographystyle{elsarticle-num} 
\bibliography{biblio}

\end{document}
\endinput
